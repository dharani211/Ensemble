I integrated NdLinear—a structure-preserving, next-generation linear transformation layer—into three models and compared its performance against nn.Linear baseline layers. I applied NdLinear to a Vision Transformer (ViT) for image classification on images of CIFAR-10, an MLP for tabular data classification on the UCI Breast Cancer dataset, and a BERT text classifier for sentiment classification. By replacing the conventional dense layers with NdLinear, I was able to preserve the native multidimensional form of the data, which resulted in a massive decrease in parameter counts, smoother training curves, and, in some instances, improved task performance.

Experiments from my work demonstrate that the NdLinear-enhanced ViT registered superior validation accuracy with fewer parameters, confirming its superiority in structured image data processing. Similarly, the NdLinear-based MLP was able to maintain feature relationships in tabular data while also guaranteeing predictive stability. In the NLP domain, the NdLinear-corrected BERT classifier exhibited well-balanced predictions and better convergence behavior. Overall, these benchmarks validate that applying NdLinear is an effective and novel approach for optimizing model performance and efficiency, entirely in sync with my project objectives.
